{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../input/3d-object-detection-for-autonomous-vehicles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=cPOtULagNnI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras.layers.merge'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ZeroPadding2D\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UpSampling2D\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m add, concatenate\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.layers.merge'"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import ZeroPadding2D\n",
    "from tensorflow.keras.layers import UpSampling2D\n",
    "from tensorflow.keras.layers.merge import add, concatenate\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _conv_block(inp, convs, skip=True):\n",
    "    x = inp\n",
    "    count = 0\n",
    "    for conv in convs:\n",
    "        if count == (len(convs) - 2) and skip:\n",
    "            skip_connection = x\n",
    "        count += 1\n",
    "        if conv['stride'] > 1: x = ZeroPadding2D(((1,0),(1,0)))(x) # peculiar padding as darknet prefer left and top\n",
    "        x = Conv2D(conv['filter'],\n",
    "                    conv['kernel'],\n",
    "                    strides=conv['stride'],padding='valid' if conv['stride'] > 1 else 'same', # peculiar padding as darknet prefer left and top\n",
    "                    name='conv_' + str(conv['layer_idx']),\n",
    "                    use_bias=False if conv['bnorm'] else True)(x)\n",
    "        if conv['bnorm']: x = BatchNormalization(epsilon=0.001, name='bnorm_' + str(conv['layer_idx']))(x)\n",
    "        if conv['leaky']: x = LeakyReLU(alpha=0.1, name='leaky_' + str(conv['layer_idx']))(x)\n",
    "        return add([skip_connection, x]) if skip else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is a Python function named _conv_block that defines a convolutional block in a deep neural network. The block consists of multiple convolutional layers with optional batch normalization, leaky ReLU activation, and skip connections.\n",
    "# Function Parameters:\n",
    "# inp: The input tensor to the convolutional block.\n",
    "# convs: A list of dictionaries, where each dictionary defines the parameters for a convolutional layer.\n",
    "# skip: A boolean flag indicating whether to use skip connections (default: True).\n",
    "# Function Body:\n",
    "# Initialize variables: Initializes the input tensor x to the input inp and a counter count to 0.\n",
    "# Loop through convolutional layers: Iterates through the list of convolutional layer parameters convs.\n",
    "# Create skip connection: If skip is True and the current layer is the second-to-last layer, creates a skip connection by storing the current tensor x in the variable skip_connection.\n",
    "# Apply convolutional layer: Applies a convolutional layer with the specified parameters (filter, kernel, stride, padding, etc.) to the current tensor x.\n",
    "# Apply batch normalization (optional): If the current layer has batch normalization enabled, applies batch normalization to the tensor x.\n",
    "# Apply leaky ReLU activation (optional): If the current layer has leaky ReLU activation enabled, applies leaky ReLU activation to the tensor x.\n",
    "# Return output: Returns the output tensor x if skip is False, otherwise returns the sum of the output tensor x and the skip connection tensor skip_connection.\n",
    "# Example Usage:\n",
    "# Python\n",
    "# convs = [\n",
    "#     {'filter': 32, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 0},\n",
    "#     {'filter': 64, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 1},\n",
    "#     {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 2}\n",
    "# ]\n",
    "\n",
    "# inp = Input(shape=(256, 256, 3))\n",
    "# x = _conv_block(inp, convs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create a YOLOv3 Keras model and save it to file\n",
    "# based on https://github.com/experiencor/keras-yolo3\n",
    "\n",
    " \n",
    "    \n",
    "\n",
    "def make_yolov3_model():\n",
    "    input_image = Input(shape=(None, None, 3))\n",
    "    # Layer  0 => 4\n",
    "    x = _conv_block(input_image, [{'filter': 32, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 0},\n",
    "                                  {'filter': 64, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 1},\n",
    "                                  {'filter': 32, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 2},\n",
    "                                  {'filter': 64, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 3}])\n",
    "    # Layer  5 => 8\n",
    "    x = _conv_block(x, [{'filter': 128, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 5},\n",
    "                        {'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 6},\n",
    "                        {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 7}])\n",
    "    # Layer  9 => 11\n",
    "    x = _conv_block(x, [{'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 9},\n",
    "                        {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 10}])\n",
    "    # Layer 12 => 15\n",
    "    x = _conv_block(x, [{'filter': 256, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 12},\n",
    "                        {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 13},\n",
    "                        {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 14}])\n",
    "    # Layer 16 => 36\n",
    "    for i in range(7):\n",
    "        x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 16+i*3},\n",
    "                            {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 17+i*3}])\n",
    "    skip_36 = x\n",
    "    # Layer 37 => 40\n",
    "    x = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 37},\n",
    "                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 38},\n",
    "                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 39}])\n",
    "    # Layer 41 => 61\n",
    "    for i in range(7):\n",
    "        x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 41+i*3},\n",
    "                            {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 42+i*3}])\n",
    "    skip_61 = x\n",
    "    # Layer 62 => 65\n",
    "    x = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 62},\n",
    "                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 63},\n",
    "                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 64}])\n",
    "    # Layer 66 => 74\n",
    "    for i in range(3):\n",
    "        x = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 66+i*3},\n",
    "                            {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 67+i*3}])\n",
    "    # Layer 75 => 79\n",
    "    x = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 75},\n",
    "                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 76},\n",
    "                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 77},\n",
    "                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 78},\n",
    "                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 79}], skip=False)\n",
    "    # Layer 80 => 82\n",
    "    yolo_82 = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 80},\n",
    "                              {'filter':  255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 81}], skip=False)\n",
    "    # Layer 83 => 86\n",
    "    x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 84}], skip=False)\n",
    "    x = UpSampling2D(2)(x)\n",
    "    x = concatenate([x, skip_61])\n",
    "    # Layer 87 => 91\n",
    "    x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 87},\n",
    "                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 88},\n",
    "                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 89},\n",
    "                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 90},\n",
    "                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 91}], skip=False)\n",
    "    # Layer 92 => 94\n",
    "    yolo_94 = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 92},\n",
    "                              {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 93}], skip=False)\n",
    "    # Layer 95 => 98\n",
    "    x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True,   'layer_idx': 96}], skip=False)\n",
    "    x = UpSampling2D(2)(x)\n",
    "    x = concatenate([x, skip_36])\n",
    "    # Layer 99 => 106\n",
    "    yolo_106 = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 99},\n",
    "                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 100},\n",
    "                               {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 101},\n",
    "                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 102},\n",
    "                               {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 103},\n",
    "                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 104},\n",
    "                               {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 105}], skip=False)\n",
    "    model = Model(input_image, [yolo_82, yolo_94, yolo_106])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "label_map = {\n",
    "    \"person\": \"blue\",\n",
    "    \"bicycle\": \"yellow\", \n",
    "    \"car\": \"red\",\n",
    "    \"truck\": \"green\",\n",
    "    \"motorbike\": \"white\", \n",
    "    \"aeroplane\": \"white\", \n",
    "    \"bus\": \"white\",\n",
    "    \"train\": \"white\", \n",
    "    \"boat\": \"white\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xmin, ymin, xmax, ymax: The coordinates of the bounding box.\n",
    "# objness: The objectness score of the bounding box (optional).\n",
    "# classes: The class probabilities of the bounding box (optional).\n",
    "# label: The predicted label of the bounding box (initialized to -1).\n",
    "# score: The predicted score of the bounding box (initialized to -1).\n",
    "# Methods:\n",
    "# __init__:\n",
    "# The constructor initializes the bounding box with the given coordinates and optional objectness and class probabilities.\n",
    "# get_label:\n",
    "# Returns the predicted label of the bounding box. If the label is not already computed, it calculates the label by taking the argmax of the class probabilities.\n",
    "# get_score:\n",
    "# Returns the predicted score of the bounding box. If the score is not already computed, it calculates the score by taking the class probability corresponding to the predicted label.\n",
    "# Example Usage:\n",
    "# Python\n",
    "# # Create a bounding box\n",
    "# box = BoundBox(10, 20, 30, 40, objness=0.8, classes=[0.1, 0.2, 0.7])\n",
    "\n",
    "# # Get the predicted label\n",
    "# label = box.get_label()\n",
    "# print(label)  # Output: 2\n",
    "\n",
    "# # Get the predicted score\n",
    "# score = box.get_score()\n",
    "# print(score)  # Output: 0.7\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundBox:\n",
    "    def __init__(self, xmin, ymin, xmax, ymax, objness = None, classes = None):\n",
    "        self.xmin = xmin\n",
    "        self.ymin = ymin\n",
    "        self.xmax = xmax\n",
    "        self.ymax = ymax\n",
    "        self.objness = objness\n",
    "        self.classes = classes\n",
    "        self.label = -1\n",
    "        self.score = -1\n",
    "\n",
    "    def get_label(self):\n",
    "        if self.label == -1:\n",
    "            self.label = np.argmax(self.classes)\n",
    "        return self.label\n",
    "\n",
    "    def get_score(self):\n",
    "        if self.score == -1:\n",
    "            self.score = self.classes[self.get_label()]\n",
    "        return self.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a Python function named decode_netout that decodes the output of a neural network, specifically a YOLO (You Only Look Once) object detection model. Here's a breakdown of the function:\n",
    "# Purpose:\n",
    "# The function takes the output of a YOLO model and converts it into a list of bounding boxes, each representing a detected object.\n",
    "# Parameters:\n",
    "# netout: The output of the YOLO model, a 4D numpy array.\n",
    "# anchors: A list of anchor boxes, used to calculate the bounding box coordinates.\n",
    "# obj_thresh: The objectness threshold, used to filter out low-confidence detections.\n",
    "# net_h and net_w: The height and width of the input image, used to scale the bounding box coordinates.\n",
    "# Step-by-Step Explanation:\n",
    "# Reshape the output: Reshapes the netout array to have shape (grid_h, grid_w, nb_box, -1), where grid_h and grid_w are the height and width of the grid, nb_box is the number of bounding boxes per grid cell, and -1 represents the remaining dimensions.\n",
    "# Apply sigmoid activation: Applies the sigmoid activation function to the first two elements of the netout array (representing the x and y coordinates of the bounding box) and to the elements starting from the 5th position (representing the objectness score and class probabilities).\n",
    "# Calculate objectness score: Calculates the objectness score by multiplying the objectness score with the class probabilities.\n",
    "# Filter out low-confidence detections: Filters out detections with an objectness score less than the specified obj_thresh.\n",
    "# Calculate bounding box coordinates: Calculates the bounding box coordinates (x, y, w, h) using the anchor boxes and the grid cell coordinates.\n",
    "# Create BoundBox objects: Creates BoundBox objects for each detected object, containing the bounding box coordinates, objectness score, and class probabilities.\n",
    "# Return the list of bounding boxes: Returns the list of BoundBox objects.\n",
    "# Example Usage:\n",
    "# Python\n",
    "# netout = ...  # output of the YOLO model\n",
    "# anchors = ...  # list of anchor boxes\n",
    "# obj_thresh = 0.5  # objectness threshold\n",
    "# net_h, net_w = 416, 416  # input image dimensions\n",
    "\n",
    "# boxes = decode_netout(netout, anchors, obj_thresh, net_h, net_w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_netout(netout, anchors, obj_thresh, net_h, net_w):\n",
    "    grid_h, grid_w = netout.shape[:2]\n",
    "    nb_box = 3\n",
    "    netout = netout.reshape((grid_h, grid_w, nb_box, -1))\n",
    "    nb_class = netout.shape[-1] - 5\n",
    "    boxes = []\n",
    "    netout[..., :2]  = _sigmoid(netout[..., :2])\n",
    "    netout[..., 4:]  = _sigmoid(netout[..., 4:])\n",
    "    netout[..., 5:]  = netout[..., 4][..., np.newaxis] * netout[..., 5:]\n",
    "    netout[..., 5:] *= netout[..., 5:] > obj_thresh\n",
    "    for i in range(grid_h*grid_w):\n",
    "        row = i / grid_w\n",
    "        col = i % grid_w\n",
    "        for b in range(nb_box):\n",
    "            # 4th element is objectness score\n",
    "            objectness = netout[int(row)][int(col)][b][4]\n",
    "            if(objectness.all() <= obj_thresh): continue\n",
    "            # first 4 elements are x, y, w, and h\n",
    "            x, y, w, h = netout[int(row)][int(col)][b][:4]\n",
    "            x = (col + x) / grid_w # center position, unit: image width\n",
    "            y = (row + y) / grid_h # center position, unit: image height\n",
    "            w = anchors[2 * b + 0] * np.exp(w) / net_w # unit: image width\n",
    "            h = anchors[2 * b + 1] * np.exp(h) / net_h # unit: image height\n",
    "            # last elements are class probabilities\n",
    "            classes = netout[int(row)][col][b][5:]\n",
    "            box = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, objectness, classes)\n",
    "            boxes.append(box)\n",
    "\n",
    "    return boxes\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a Python function named correct_yolo_boxes that corrects the bounding box coordinates predicted by a YOLO (You Only Look Once) object detection model. Here's a breakdown of the function:\n",
    "# Purpose:\n",
    "# The function takes the predicted bounding boxes, image dimensions, and network dimensions as input, and corrects the bounding box coordinates to match the original image dimensions.\n",
    "# Parameters:\n",
    "# boxes: A list of BoundBox objects, representing the predicted bounding boxes.\n",
    "# image_h and image_w: The height and width of the original image.\n",
    "# net_h and net_w: The height and width of the network's input image.\n",
    "# Step-by-Step Explanation:\n",
    "# Calculate scaling factors: Calculates the scaling factors x_scale and y_scale to convert the network's output coordinates to the original image coordinates.\n",
    "# Calculate offset factors: Calculates the offset factors x_offset and y_offset to account for the difference in image dimensions.\n",
    "#     This is a Python function named correct_yolo_boxes that corrects the bounding box coordinates predicted by a YOLO (You Only Look Once) object detection model. Here's a breakdown of the function:\n",
    "# Purpose:\n",
    "# The function takes the predicted bounding boxes, image dimensions, and network dimensions as input, and corrects the bounding box coordinates to match the original image dimensions.\n",
    "# Parameters:\n",
    "# boxes: A list of BoundBox objects, representing the predicted bounding boxes.\n",
    "# image_h and image_w: The height and width of the original image.\n",
    "# net_h and net_w: The height and width of the network's input image.\n",
    "# Step-by-Step Explanation:\n",
    "# Calculate scaling factors: Calculates the scaling factors x_scale and y_scale to convert the network's output coordinates to the original image coordinates.\n",
    "# Calculate offset factors: Calculates the offset factors x_offset and y_offset to account for the difference in image dimensions.\n",
    "#     boxes = ...  # list of BoundBox objects\n",
    "# image_h, image_w = 416, 416  # original image dimensions\n",
    "# net_h, net_w = 416, 416  # network's input image dimensions\n",
    "\n",
    "# correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w):\n",
    "    new_w, new_h = net_w, net_h\n",
    "    for i in range(len(boxes)):\n",
    "        x_offset, x_scale = (net_w - new_w)/2./net_w, float(new_w)/net_w\n",
    "        y_offset, y_scale = (net_h - new_h)/2./net_h, float(new_h)/net_h\n",
    "        boxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)\n",
    "        boxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)\n",
    "        boxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)\n",
    "        boxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is a Python function named _interval_overlap that calculates the overlap between two intervals. Here's a breakdown of the function:\n",
    "# Purpose:\n",
    "# The function takes two intervals, interval_a and interval_b, as input and returns the length of their overlap.\n",
    "# Parameters:\n",
    "# interval_a: The first interval, represented as a tuple (x1, x2).\n",
    "# interval_b: The second interval, represented as a tuple (x3, x4).\n",
    "# Step-by-Step Explanation:\n",
    "# Unpack intervals: Unpacks the intervals into their respective start and end points: x1, x2, x3, and x4.\n",
    "# Check for no overlap: Checks if the intervals do not overlap by verifying if one interval ends before the other starts. If so, returns 0.\n",
    "# Calculate overlap: Calculates the overlap by finding the minimum of the two end points (x2 and x4) and subtracting the maximum of the two start points (x1 and x3).\n",
    "# Example Usage:\n",
    "# Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _interval_overlap(interval_a, interval_b):\n",
    "    x1, x2 = interval_a\n",
    "    x3, x4 = interval_b\n",
    "    if x3 < x1:\n",
    "        if x4 < x1:\n",
    "            return 0\n",
    "        else:\n",
    "            return min(x2,x4) - x1\n",
    "    else:\n",
    "        if x2 < x3:\n",
    "            return 0\n",
    "        else:\n",
    "            return min(x2,x4) - x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a Python function named bbox_iou that calculates the Intersection over Union (IoU) between two bounding boxes. Here's a breakdown of the function:\n",
    "# Purpose:\n",
    "# The function takes two bounding boxes, box1 and box2, as input and returns their IoU score.\n",
    "# Parameters:\n",
    "# box1 and box2: Two bounding boxes, each with attributes xmin, xmax, ymin, and ymax.\n",
    "# Step-by-Step Explanation:\n",
    "# Calculate intersection width: Calculates the intersection width between the two boxes using the _interval_overlap function.\n",
    "# Calculate intersection height: Calculates the intersection height between the two boxes using the _interval_overlap function.\n",
    "# Calculate intersection area: Calculates the intersection area by multiplying the intersection width and height.\n",
    "# Calculate box areas: Calculates the areas of both boxes by multiplying their widths and heights.\n",
    "# Calculate union area: Calculates the union area by adding the areas of both boxes and subtracting the intersection area.\n",
    "# Calculate IoU: Calculates the IoU by dividing the intersection area by the union area.\n",
    "# IoU Interpretation:\n",
    "# The IoU score ranges from 0 to 1, where:\n",
    "# 0: No overlap between the boxes.\n",
    "# 1: Complete overlap between the boxes.\n",
    "# Values between 0 and 1: Partial overlap between the boxes.\n",
    "# Example Usage:\n",
    "# Python\n",
    "# box1 = BoundBox(10, 20, 30, 40)\n",
    "# box2 = BoundBox(20, 30, 40, 50)\n",
    "\n",
    "# iou = bbox_iou(box1, box2)\n",
    "# print(iou)  # Output: IoU score between 0 and 1\n",
    "\n",
    "# Ask Meta AI anything...\n",
    "# Messages are generated by AI and may b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_iou(box1, box2):\n",
    "    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n",
    "    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])\n",
    "    intersect = intersect_w * intersect_h\n",
    "    w1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n",
    "    w2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n",
    "    union = w1*h1 + w2*h2 - intersect\n",
    "    return float(intersect) / union\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def do_nms(boxes, nms_thresh):\n",
    "    if len(boxes) > 0:\n",
    "        nb_class = len(boxes[0].classes)\n",
    "    else:\n",
    "        return\n",
    "    for c in range(nb_class):\n",
    "        sorted_indices = np.argsort([-box.classes[c] for box in boxes])\n",
    "        for i in range(len(sorted_indices)):\n",
    "            index_i = sorted_indices[i]\n",
    "            if boxes[index_i].classes[c] == 0: continue\n",
    "            for j in range(i+1, len(sorted_indices)):\n",
    "                index_j = sorted_indices[j]\n",
    "                if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:\n",
    "                    boxes[index_j].classes[c] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import expand_dims\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_pixels(filename, shape):\n",
    "    # load the image to get its shape\n",
    "    image = load_img(filename)\n",
    "    width, height = image.size\n",
    "    # load the image with the required size\n",
    "    image = load_img(filename, target_size=shape)\n",
    "    # convert to numpy array\n",
    "    image = img_to_array(image)\n",
    "    # scale pixel values to [0, 1]\n",
    "    image = image.astype('float32')\n",
    "    image /= 255.0\n",
    "    # add a dimension so that we have one sample\n",
    "    image = expand_dims(image, 0)\n",
    "    \n",
    "    return image, width, height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # get all of the results above a threshold\n",
    "def get_boxes(boxes, labels, thresh):\n",
    "    v_boxes, v_labels, v_scores = list(), list(), list()\n",
    "    # enumerate all boxes\n",
    "    for box in boxes:\n",
    "        # enumerate all possible labels\n",
    "        for i in range(len(labels)):\n",
    "            # check if the threshold for this label is high enough\n",
    "            if box.classes[i] > thresh:\n",
    "                v_boxes.append(box)\n",
    "                v_labels.append(labels[i])\n",
    "                v_scores.append(box.classes[i]*100)\n",
    "                # don't break, many labels may trigger for one box\n",
    "    return v_boxes, v_labels, v_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare an imag# draw all results\n",
    "def draw_boxes(filename, v_boxes, v_labels, v_scores):\n",
    "    # load the image\n",
    "    data = pyplot.imread(filename)\n",
    "    # plot the image\n",
    "    pyplot.imshow(data)\n",
    "    # get the context for drawing boxes\n",
    "    ax = pyplot.gca()\n",
    "    # plot each box\n",
    "    for i in range(len(v_boxes)):\n",
    "        box = v_boxes[i]\n",
    "        # get coordinates\n",
    "        y1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax\n",
    "        # calculate width and height of the box\n",
    "        width, height = x2 - x1, y2 - y1\n",
    "        # create the shape\n",
    "        rect = Rectangle((x1, y1), width, height, fill=False, color=label_map[v_labels[i]])\n",
    "        # draw the box\n",
    "        ax.add_patch(rect)\n",
    "        # draw text and score in top left corner\n",
    "        label = \"%s (%.3f)\" % (v_labels[i], v_scores[i])\n",
    "        pyplot.text(x1, y1, label, color=label_map[v_labels[i]])\n",
    "    # show the plot\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightReader:\n",
    "    def __init__(self, weight_file):\n",
    "        self.weight_file = weight_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = make_yolov3_model()\n",
    "\n",
    "# load the model weights\n",
    "# I have loaded the pretrained weights in a separate dataset\n",
    "weight_reader = WeightReader('../input/lyft-3d-recognition/yolov3.weights')\n",
    "\n",
    "# set the model weights into the model\n",
    "weight_reader.load_weights(model)\n",
    "\n",
    "# save the model to file\n",
    "\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load yolov3 model\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters used in the Dataset, on which YOLOv3 was pretrained\n",
    "anchors = [[116,90, 156,198, 373,326], [30,61, 62,45, 59,119], [10,13, 16,30, 33,23]]\n",
    "\n",
    "# define the expected input shape for the model\n",
    "WIDTH, HEIGHT = 416, 416\n",
    "\n",
    "# define the probability threshold for detected objects\n",
    "class_threshold = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "images = os.listdir('../input/3d-object-detection-for-autonomous-vehicles/train_images')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import expand_dims\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# load and prepare an image\n",
    "def load_image_pixels(filename, shape):\n",
    "    '''\n",
    "    Function preprocess the images to 416x416, which is the standard input shape for YOLOv3, \n",
    "    and also keeps track of the originl shape, which is later used to draw the boxes.\n",
    "    \n",
    "    paramters:\n",
    "    filename {String}: path to the image\n",
    "    shape {tuple}: shape of the input dimensions of the network\n",
    "    \n",
    "    returns:image {PIL}: image of shape 'shape'\n",
    "    width {int}: original width of the picture\n",
    "    height {int}: original height of the picture\n",
    "    '''\n",
    "    # load the image to get its shape\n",
    "    image = load_img(filename)\n",
    "    width, height = image.size\n",
    "    \n",
    "    # load the image with the required size\n",
    "    image = load_img(filename, target_size=shape)\n",
    "    \n",
    "    # convert to numpy arrayimage = img_to_array(image)\n",
    "    \n",
    "    # scale pixel values to [0, 1]\n",
    "    image = image.astype('float32')\n",
    "    image /= 255.0\n",
    "    \n",
    "    # add a dimension so that we have one sampleimage = expand_dims(image, 0)\n",
    "    return image, width, height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in images:\n",
    "    photo_filename = DATA_PATH + 'train_images/' + file\n",
    "    \n",
    "    # load picture with old dimensions\n",
    "    image, image_w, image_h = load_image_pixels(photo_filename, (WIDTH, HEIGHT))\n",
    "    \n",
    "    # Predict image\n",
    "    yhat = model.predict(image)\n",
    "    \n",
    "    # Create boxesboxes = list()\n",
    "    for i in range(len(yhat)):\n",
    "        # decode the output of the network\n",
    "        boxes += decode_netout(yhat[i][0], anchors[i], class_threshold, HEIGHT, WIDTH)\n",
    "\n",
    "    # correct the sizes of the bounding boxes for the shape of the image\n",
    "    correct_yolo_boxes(boxes, image_h, image_w, HEIGHT, WIDTH)\n",
    "\n",
    "    # suppress non-maximal boxes\n",
    "    do_nms(boxes, 0.5)# define the labels (Filtered only the ones relevant for this task, which were used in pretraining the YOLOv3 model)\n",
    "    labels = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\",\"boat\"]\n",
    "\n",
    "    # get the details of the detected objects\n",
    "    v_boxes, v_labels, v_scores = get_boxes(boxes, labels, class_threshold)\n",
    "\n",
    "    # draw what we found \n",
    "    draw_boxes(photo_filename, v_boxes, v_labels, v_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
